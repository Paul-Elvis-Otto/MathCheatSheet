---
title: "Math for data science R cheatsheet"
format: html
execute:
  freeze: auto
output-dir: docs
toc: true
---
# Hello!
This is the unofficial math for data science cheat sheet


# The basics




# `summary()` for Linear Models in R

The `summary()` function is a powerful tool for summarizing linear models (`lm`). This cheat sheet explains the key components of the `summary()` output and how to interpret them.



### Syntax:
```{{R}}
summary(lm_model)
```

### Example:

```{r}
# Fit a simple linear regression model
data(mtcars)
lm_model <- lm(mpg ~ hp, data = mtcars)

# Get the summary
summary(lm_model)
```

## Key Sections in `summary()` Output

### (a) Call

The first line repeats the formula used in the model:

Call:
lm(formula = mpg ~ hp, data = mtcars)

	•	Interpretation: Indicates that mpg (miles per gallon) is the response variable and hp (horsepower) is the predictor.

### (b) Residuals

A summary of the residuals (errors between observed and predicted values):

Residuals:
    Min      1Q  Median      3Q     Max
-5.7121 -2.1122 -0.8854  1.5819  8.2360

	•	Min, 1Q, Median, 3Q, Max:
	•	These represent the spread of residuals.
	•	Ideally, the median should be close to 0, and residuals should be symmetrically distributed around 0.
	•	Large residuals (outliers) can indicate poor model fit.

### (c) Coefficients

The most important section of the output:

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***
hp          -0.06823    0.01012  -6.742 1.79e-07 ***

Columns:
	1.	Estimate:
	•	The fitted coefficients for the intercept and predictors:
	•	Intercept: 30.09886 (predicted mpg when hp = 0).
	•	Slope for hp: -0.06823 (for every 1-unit increase in hp, mpg decreases by ~0.06823).
	2.	Std. Error:
	•	The standard error of the coefficient estimates, which measures their variability.
	•	Smaller standard errors indicate more precise estimates.
	3.	t value:
	•	( t )-statistic for testing if the coefficient is significantly different from 0:
[
t = \frac{\text{Estimate}}{\text{Std. Error}}
]
	4.	Pr(>|t|):
	•	The p-value for the null hypothesis (( H_0: \beta = 0 )).
	•	Smaller values (< 0.05) indicate that the predictor is statistically significant.
	5.	Significance Codes:
	•	Indicate the strength of the p-value:

‘***’ < 0.001, ‘**’ < 0.01, ‘*’ < 0.05


	•	In this example, both the intercept and hp are highly significant.

### (d) Residual Standard Error

Residual standard error: 3.863 on 30 degrees of freedom

	•	Measures the typical size of residuals (in units of the response variable).
	•	Smaller values indicate a better fit.

### (e) ( R^2 ) and Adjusted ( R^2 )

Multiple R-squared:  0.6024,    Adjusted R-squared:  0.5892

	1.	( R^2 ):
	•	Proportion of variance in the response variable (mpg) explained by the predictor (hp).
	•	Here, ( R^2 = 0.6024 ), meaning ~60.24% of the variance in mpg is explained by hp.
	2.	Adjusted ( R^2 ):
	•	Adjusted for the number of predictors in the model.
	•	Penalizes adding unnecessary predictors.

### (f) F-Statistic

F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07

	•	Tests the null hypothesis that all coefficients (except the intercept) are 0.
	•	Large ( F )-values and small p-values indicate a good overall fit.
	•	Degrees of Freedom: Indicates the number of predictors (1) and residual degrees of freedom (30).

## How to Interpret Results
	1.	Significance of Predictors:
	•	Look at the p-values (Pr(>|t|)) for each coefficient.
	•	If ( p < 0.05 ), the predictor is statistically significant.
	2.	Model Fit:
	•	Check ( R^2 ) and Adjusted ( R^2 ):
	•	Higher values indicate better fit.
	3.	Residuals:
	•	Ensure residuals are symmetrically distributed around 0 and have small variability.
	4.	Practical Significance:
	•	Even if a predictor is statistically significant, check its effect size (Estimate).

Example Code

```{r}
# Load dataset
data(mtcars)

# Fit linear model
lm_model <- lm(mpg ~ hp, data = mtcars)

# Get summary
summary(lm_model)
```






Here’s a cheat sheet for plotting variables in R, covering density plots, histograms, and bivariate relationships:


# Plotting Variables in R


## 1. Plotting a Density Curve

Density plots visualize the distribution of a continuous variable.

### **Syntax**

```r
plot(density(variable), main = "Density Plot", xlab = "Variable", ylab = "Density")
```

### Example

```{r}
# Example data
set.seed(1)
data <- rnorm(100)

# Density plot
plot(density(data),
     main = "Density Plot of Data",
     xlab = "Value",
     ylab = "Density",
     col = "blue",
     lwd = 2)
```

## Plotting a Histogram

Histograms represent the frequency distribution of a continuous variable.

### Syntax
```{{r}}
hist(variable, breaks = n, main = "Histogram", xlab = "Variable", col = "color")
```

### Example

```{r}
# Histogram
hist(data,
     breaks = 10,  # Number of bins
     main = "Histogram of Data",
     xlab = "Value",
     col = "lightblue",
     border = "black")
```

## Scatterplot with a Fitted Line or Curve

Scatterplots show the relationship between two variables. Adding a line or curve highlights the trend.

### syntax

```{{r}}
plot(x, y, main = "Scatterplot", xlab = "X", ylab = "Y", pch = 19)
```


4. Combined Example

Below is an example showing how to create all three plots for a dataset.

```{r}
# Generate example data
set.seed(123)
x <- rnorm(100)
y <- 2 + 1.5 * x + rnorm(100, sd = 1)

# (1) Density Plot
plot(density(x),
     main = "Density Plot of X",
     xlab = "X",
     col = "blue",
     lwd = 2)

# (2) Histogram
hist(x,
     breaks = 10,
     main = "Histogram of X",
     xlab = "X",
     col = "lightblue",
     border = "black")

# (3) Scatterplot with Fitted Line
plot(x, y,
     main = "Scatterplot with Fitted Line",
     xlab = "X",
     ylab = "Y",
     pch = 19,
     col = "blue")
abline(lm(y ~ x), col = "red", lwd = 2)

```


# Simulating Data and Creating a Simulation Distribution

In this document, we simulate data from a linear model, use a loop to calculate a quantity of interest (e.g., the mean of the response variable), and create a simulation distribution from repeated iterations.



## Generating Simulated Data

We generate data using the following linear model:
$$
y = \beta_0 + \beta_1 x + \epsilon
$$

where:

- $x \sim N(0, 1)$

- $\epsilon \sim N(0, \sigma^2)$

### **Code**

```{r}
set.seed(123)  # For reproducibility

# Parameters
n <- 100                 # Number of observations
beta_0 <- 2              # Intercept
beta_1 <- 1.5            # Slope
sigma <- 1               # Standard deviation of noise

# Generate data
x <- rnorm(n, mean = 0, sd = 1)  # Predictor variable
epsilon <- rnorm(n, mean = 0, sd = sigma)  # Random noise
y <- beta_0 + beta_1 * x + epsilon         # Response variable

# Create a data frame
data <- data.frame(x = x, y = y)

# Plot the simulated data
plot(data$x, data$y,
     main = "Simulated Data",
     xlab = "X",
     ylab = "Y",
     pch = 19,
     col = "blue")
abline(a = beta_0, b = beta_1, col = "red", lwd = 2)  # True line
```


## Simulation: Mean of the Response Variable

Now, we will simulate 1000 datasets based on the same model and compute the mean of ( y ) for each dataset. This will generate a distribution of the mean of ( y ).

```{r}
set.seed(123)

# Simulation parameters
num_simulations <- 1000
n <- 100
beta_0 <- 2
beta_1 <- 1.5
sigma <- 1

# Storage for means
y_means <- numeric(num_simulations)

# Simulation loop
for (i in 1:num_simulations) {
  # Generate x and epsilon
  x <- rnorm(n, mean = 0, sd = 1)
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  
  # Generate y
  y <- beta_0 + beta_1 * x + epsilon
  
  # Compute and store the mean of y
  y_means[i] <- mean(y)
}

# Plot the simulation distribution
hist(y_means,
     breaks = 30,
     main = "Simulation Distribution of Mean of Y",
     xlab = "Mean of Y",
     col = "lightblue",
     border = "black")

```

## Analyzing the Simulation Distribution

We calculate key statistics of the simulation distribution (mean and standard deviation) and compare them to theoretical expectations.

```{r}

# Summary statistics
simulated_mean <- mean(y_means)
simulated_sd <- sd(y_means)

# Print results
cat("Simulated Mean of Y:", simulated_mean, "\n")
cat("Simulated SD of Y:", simulated_sd, "\n")

# Compare to theoretical mean and standard deviation
theoretical_mean <- beta_0
theoretical_sd <- sigma / sqrt(n)

cat("Theoretical Mean of Y:", theoretical_mean, "\n")
cat("Theoretical SD of Y:", theoretical_sd, "\n")
```



## Combined Code for Simulation

```{r}
# Set parameters
set.seed(123)
num_simulations <- 1000
n <- 100
beta_0 <- 2
beta_1 <- 1.5
sigma <- 1

# Storage for means
y_means <- numeric(num_simulations)

# Simulation loop
for (i in 1:num_simulations) {
  x <- rnorm(n, mean = 0, sd = 1)
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  y <- beta_0 + beta_1 * x + epsilon
  y_means[i] <- mean(y)
}

# Plot simulation distribution
hist(y_means,
     breaks = 30,
     main = "Simulation Distribution of Mean of Y",
     xlab = "Mean of Y",
     col = "lightblue",
     border = "black")

# Calculate and print statistics
cat("Simulated Mean of Y:", mean(y_means), "\n")
cat("Simulated SD of Y:", sd(y_means), "\n")
cat("Theoretical Mean of Y:", beta_0, "\n")
cat("Theoretical SD of Y:", sigma / sqrt(n), "\n")
```


## Key Takeaways
- Simulation Loop: Use a for loop to repeat the simulation and calculate quantities of interest (e.g., the mean of ( y )).
- Simulation Distribution: The distribution of the simulated means approximates the theoretical mean and variance.
- Practical Applications: This technique is useful for hypothesis testing, bootstrapping, and exploring variability in statistical models.




# Linear Regression Models and Regularization

This document demonstrates how to fit linear regression models (including interaction and polynomial terms) and perform Lasso and Ridge regularization.


## Generating Simulated Data

We simulate a dataset based on the following model:
$$
y = 2 + 1.5x_1 - 2x_2 + 0.8x_1x_2 + 1.2x_1^2 + \epsilon
$$
where:

- $x_1$ and $x_2$ are predictors drawn from $N(0, 1)$
- $\epsilon \sim N(0, 1)$ is random noise.


### Code:

```{r}
set.seed(123)
# Generate data
n <- 200
x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 0, sd = 1)
epsilon <- rnorm(n, mean = 0, sd = 1)
y <- 2 + 1.5 * x1 - 2 * x2 + 0.8 * x1 * x2 + 1.2 * x1^2 + epsilon
# Create a data frame
data <- data.frame(y, x1, x2)
```

## Fitting a Linear Regression Model

We include interaction $x_1x_2$ and polynomial $x_1^2$ terms in the regression.


```{r}
# Fit linear model with interactions and polynomials
lm_model <- lm(y ~ x1 + x2 + I(x1 * x2) + I(x1^2), data = data)

# Print summary of the model
summary(lm_model)
```

**Interpretation:**
- Look at the coefficients to assess the relationship between predictors and the response.
- The $R^2$ value shows how much variance is explained by the model.

## Adding Regularization: Ridge and Lasso

For regularized regression, we use the `glmnet` package.

### Ridge Regression

Ridge regression minimizes the sum of squared residuals with an added penalty proportional to the sum of squared coefficients $\lambda \sum \beta_j^2$.

```{r}

library(glmnet)

# Prepare data for glmnet
x <- model.matrix(y ~ x1 + x2 + I(x1 * x2) + I(x1^2), data)[, -1]  # Remove intercept
y <- data$y

# Fit Ridge Regression
ridge_model <- glmnet(x, y, alpha = 0)  # alpha = 0 for Ridge
plot(ridge_model, xvar = "lambda", label = TRUE)  # Plot coefficient paths
```


**Interpretation:**

- The plot shows how coefficients shrink as $\lambda$ increases.

- Ridge is useful when multicollinearity is present or to stabilize coefficients.

### Lasso Regression

Lasso regression minimizes the sum of squared residuals with a penalty proportional to the sum of absolute coefficients $\lambda \sum |\beta_j|$

```{r}

# Fit Lasso Regression
lasso_model <- glmnet(x, y, alpha = 1)  # alpha = 1 for Lasso
plot(lasso_model, xvar = "lambda", label = TRUE)  # Plot coefficient paths
```

**Interpretation:**

- Lasso can shrink some coefficients to exactly zero, effectively performing variable selection.

## Choosing Optimal $\lambda$ Using Cross-Validation


```{r}
# Ridge Cross-Validation
ridge_cv <- cv.glmnet(x, y, alpha = 0)  # Ridge
best_lambda_ridge <- ridge_cv$lambda.min
cat("Best Lambda for Ridge:", best_lambda_ridge, "\n")

# Lasso Cross-Validation
lasso_cv <- cv.glmnet(x, y, alpha = 1)  # Lasso
best_lambda_lasso <- lasso_cv$lambda.min
cat("Best Lambda for Lasso:", best_lambda_lasso, "\n")
```

**Interpretation:**

- `cv.glmnet` finds the optimal $\lambda$ using cross-validation.

- Use the best $\lambda$ to refit the Ridge or Lasso model for predictions.

## Comparison of Models


```{r}
# Refit models with optimal lambda
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lambda_ridge)
lasso_final <- glmnet(x, y, alpha = 1, lambda = best_lambda_lasso)

# Coefficients for Ridge and Lasso
coef_ridge <- coef(ridge_final)
coef_lasso <- coef(lasso_final)

cat("Ridge Coefficients:\n")
print(coef_ridge)
cat("Lasso Coefficients:\n")
print(coef_lasso)
```

**Interpretation:**

- Compare the coefficients from Ridge and Lasso with those from the unregularized linear model.

- Ridge retains all variables, but shrinks coefficients. Lasso may set some coefficients to zero.

## Summary

| Model         | Key Feature                                             | Use Case                                   |
|---------------|---------------------------------------------------------|-------------------------------------------|
| Linear Model  | Includes interactions and polynomials.                  | Explains relationships between variables. |
| Ridge         | Shrinks coefficients toward zero; no variable elimination. | Addresses multicollinearity or overfitting. |
| Lasso         | Shrinks coefficients, can eliminate variables.          | Performs variable selection and regularization. |








# Matrix Operations in R Cheat Sheet

This cheat sheet covers essential matrix operations in R, including creating matrices, checking dimensions, and performing common operations like addition, multiplication, transpose, and inverse.


## 1. Creating a Matrix

### **Using `matrix()`**

```{r}
# Create a 2x3 matrix
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)
print(A)
```

|Function|	Description|	Example|
|---|---|---|
|`matrix()`|	Creates a matrix|	`matrix(1:6, nrow = 2)`|
|`rbind()`|	Binds rows to form a matrix|	`rbind(c(1, 2), c(3, 4))`|
|`cbind()`|	Binds columns to form a matrix|	`cbind(c(1, 3), c(2, 4))`|

## checking dimensions

### Useful Functions

|Function|	Description|	Example|
|---|---|---|
|`dim()`|	Dimensions of the matrix|	`dim(A)`|
|`nrow()`|	Number of rows|	`nrow(A)`|
|`ncol()`|	Number of columns|	`ncol(A)`|


```{r}
dim(A)       # Dimensions of matrix A
nrow(A)      # Number of rows in A
ncol(A)      # Number of columns in A
```

## Matrix Addition and Subtraction

**Rules**
Matrices must have the same dimensions.


```{r}
# Create two matrices
B <- matrix(c(6, 5, 4, 3, 2, 1), nrow = 2, ncol = 3)
A + B  # Matrix addition
A - B  # Matrix subtraction
```

## Matrix Multiplication

### Element-Wise Multiplication

|Operator	|Description	|Example|
|---|---|---|
|`*`	|Element-wise multiplication|	`A * B`|

**Matrix Multiplication**

- Use `%*%` for true matrix multiplication.

- Ensure the number of columns in the first matrix equals the number of rows in the second.

### Example of matrix multiplication
```{r}
C <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)
D <- matrix(c(5, 6, 7, 8), nrow = 2, ncol = 2)

C %*% D  # Matrix multiplication
```

## Transpose of a Matrix

**Function**

|Function	|Description	|Example|
|---|---|---|
|`t()`	|Transpose of a matrix|	`t(A)`|

# Transpose matrix A

```{r}
transpose_A <- t(A)
print(transpose_A)
```

## Inverse of a Matrix

**Function**

- Use `solve()` to compute the inverse of a matrix.

- The matrix must be square and non-singular (determinant ≠ 0).

```{r}
# Create a square matrix
E <- matrix(c(2, 1, 1, 2), nrow = 2)

# Compute the inverse
inverse_E <- solve(E)
print(inverse_E)
```

|Function|	Description	|Example|
|---|---|---|
|`solve()`|	Computes the inverse of a matrix	|`solve(E)`|
|`det()`|	Computes the determinant of a matrix	|`det(E)`|

## Combined Example

```{r}
# Create two matrices
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)
B <- matrix(c(6, 5, 4, 3, 2, 1), nrow = 2, ncol = 3)

# Matrix addition
add_AB <- A + B

# Matrix transpose
transpose_A <- t(A)

# Create a square matrix for inverse
C <- matrix(c(2, 1, 1, 2), nrow = 2)

# Compute the inverse
inverse_C <- solve(C)

# Print results
cat("Matrix A:\n")
print(A)
cat("\nMatrix B:\n")
print(B)
cat("\nA + B:\n")
print(add_AB)
cat("\nTranspose of A:\n")
print(transpose_A)
cat("\nInverse of C:\n")
print(inverse_C)
```

**Summary Table**

| Operation                   | Function                | Example                |
|-----------------------------|-------------------------|------------------------|
| Create a matrix             | `matrix()`             | `matrix(1:6, nrow = 2)` |
| Bind rows/columns           | `rbind()`, `cbind()`   | `rbind(c(1, 2), c(3, 4))` |
| Check dimensions            | `dim()`, `nrow()`, `ncol()` | `dim(A)`          |
| Add/Subtract matrices       | `+`, `-`               | `A + B`                |
| Element-wise multiplication | `*`                    | `A * B`                |
| Matrix multiplication       | `%*%`                  | `A %*% B`              |
| Transpose                   | `t()`                  | `t(A)`                 |
| Inverse                     | `solve()`              | `solve(C)`             |


